{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Torch will run on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nkhalil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nkhalil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Imports and set torch device\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "import torch\n",
    "import sys\n",
    "from scripts.helper_functions import segment\n",
    "import kaolin as kal\n",
    "import matplotlib.pyplot as plt\n",
    "from meshseg.models.GLIP.glip import GLIPModel\n",
    "import igl\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Torch will run on:', device)\n",
    "\n",
    "object = 'cabinet' \n",
    "obj_path = f'data/demo/{object}.obj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices:  4399\n",
      "Number of faces:  6804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f91330292094d18b20397a8ea7af34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(-7.450580â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read mesh\n",
    "mesh = kal.io.obj.import_mesh(\n",
    "    obj_path,\n",
    "    with_normals=True,\n",
    "    with_materials=False,\n",
    ")\n",
    "\n",
    "vertices_tensor = mesh.vertices.to(device)\n",
    "faces_tensor = mesh.faces.to(device)\n",
    "\n",
    "vertices = vertices_tensor.detach().cpu().numpy()\n",
    "faces = faces_tensor.detach().cpu().numpy()\n",
    "color = mesh.vertex_normals.cpu().numpy()\n",
    "print('Number of vertices: ', vertices.shape[0])\n",
    "print('Number of faces: ', faces.shape[0])\n",
    "# Visualize mesh\n",
    "mp.plot(vertices, faces, color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call SATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the mesh...\n",
      "Reading the mesh with path: data/demo\\cabinet.obj\n",
      "\thaving 6804 faces and 4399 vertices\n",
      "Sampling the mesh...\n",
      "Sampled 8798 points\n",
      "Creating the renderer...\n",
      "Random rendering: False\n",
      "Rendering the views...\n",
      "Num views: 12\n",
      "Rendering the views...done\n",
      "Initializing GLIP...\n",
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniforge3\\envs\\meshseg\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "Finished Initializing GLIP\n",
      "Per Sample Point scores\n",
      "Getting samples neighborhood\n",
      "Prompts: ['the back panel of a cabinet.', 'the bottom panel of a cabinet.', 'the drawers of a cabinet.', 'the handles of a cabinet.', 'the side panels of a cabinet.', 'the top panel of a cabinet.']\n",
      "Feeding the views to GLIP...\n",
      "Num views: 12\n",
      "GLIP - View: 0 Prompt: 0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniforge3\\envs\\meshseg\\lib\\site-packages\\transformers\\modeling_utils.py:977: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\miniforge3\\envs\\meshseg\\lib\\site-packages\\torch\\nn\\functional.py:3847: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time per image: 2.6907807000000012\n",
      "GLIP - View: 0 Prompt: 1 inference time per image: 0.5144070999999997\n",
      "GLIP - View: 0 Prompt: 2 inference time per image: 0.5283273000000008\n",
      "GLIP - View: 0 Prompt: 3 inference time per image: 0.5205507999999988\n",
      "GLIP - View: 0 Prompt: 4 inference time per image: 0.5097847999999985\n",
      "GLIP - View: 0 Prompt: 5 inference time per image: 0.5258920000000025\n",
      "GLIP - View: 1 Prompt: 0 inference time per image: 0.5128042999999991\n",
      "GLIP - View: 1 Prompt: 1 inference time per image: 0.5167497999999995\n",
      "GLIP - View: 1 Prompt: 2 inference time per image: 0.5155628000000014\n",
      "GLIP - View: 1 Prompt: 3 inference time per image: 0.5197671000000028\n",
      "GLIP - View: 1 Prompt: 4 inference time per image: 0.5336898000000012\n",
      "GLIP - View: 1 Prompt: 5 inference time per image: 0.5196092000000014\n",
      "GLIP - View: 2 Prompt: 0 inference time per image: 0.5165456000000006\n",
      "GLIP - View: 2 Prompt: 1 inference time per image: 0.5293209999999995\n",
      "GLIP - View: 2 Prompt: 2 inference time per image: 0.5191644999999987\n",
      "GLIP - View: 2 Prompt: 3 inference time per image: 0.5243165000000012\n",
      "GLIP - View: 2 Prompt: 4 inference time per image: 0.5171092000000002\n",
      "GLIP - View: 2 Prompt: 5 inference time per image: 0.5259956999999993\n",
      "GLIP - View: 3 Prompt: 0 inference time per image: 0.5197793999999973\n",
      "GLIP - View: 3 Prompt: 1 inference time per image: 0.5190848000000017\n",
      "GLIP - View: 3 Prompt: 2 inference time per image: 0.5197482000000022\n",
      "GLIP - View: 3 Prompt: 3 inference time per image: 0.5223155999999989\n",
      "GLIP - View: 3 Prompt: 4 inference time per image: 0.5221738000000045\n",
      "GLIP - View: 3 Prompt: 5 inference time per image: 0.5118435999999988\n",
      "GLIP - View: 4 Prompt: 0 inference time per image: 0.5279732999999993\n",
      "GLIP - View: 4 Prompt: 1 inference time per image: 0.5128593000000023\n",
      "GLIP - View: 4 Prompt: 2 inference time per image: 0.5165034000000048\n",
      "GLIP - View: 4 Prompt: 3 inference time per image: 0.5211800000000011\n",
      "GLIP - View: 4 Prompt: 4 inference time per image: 0.5134808999999976\n",
      "GLIP - View: 4 Prompt: 5 inference time per image: 0.515350699999999\n",
      "GLIP - View: 5 Prompt: 0 inference time per image: 0.5215764000000007\n",
      "GLIP - View: 5 Prompt: 1 inference time per image: 0.5249748000000025\n",
      "GLIP - View: 5 Prompt: 2 inference time per image: 0.5075741999999934\n",
      "GLIP - View: 5 Prompt: 3 inference time per image: 0.5171782000000036\n",
      "GLIP - View: 5 Prompt: 4 inference time per image: 0.5180495000000036\n",
      "GLIP - View: 5 Prompt: 5 inference time per image: 0.5166593000000006\n",
      "GLIP - View: 6 Prompt: 0 inference time per image: 0.5118835000000033\n",
      "GLIP - View: 6 Prompt: 1 inference time per image: 0.5096017999999987\n",
      "GLIP - View: 6 Prompt: 2 inference time per image: 0.5060015999999976\n",
      "GLIP - View: 6 Prompt: 3 inference time per image: 0.5055784999999986\n",
      "GLIP - View: 6 Prompt: 4 inference time per image: 0.5120971999999995\n",
      "GLIP - View: 6 Prompt: 5 inference time per image: 0.5232103999999964\n",
      "GLIP - View: 7 Prompt: 0 inference time per image: 0.5276467000000054\n",
      "GLIP - View: 7 Prompt: 1 inference time per image: 0.5326650000000015\n",
      "GLIP - View: 7 Prompt: 2 inference time per image: 0.5190376999999984\n",
      "GLIP - View: 7 Prompt: 3 inference time per image: 0.5227614000000003\n",
      "GLIP - View: 7 Prompt: 4 inference time per image: 0.5232790999999963\n",
      "GLIP - View: 7 Prompt: 5 inference time per image: 0.5135397999999967\n",
      "GLIP - View: 8 Prompt: 0 inference time per image: 0.5046166999999997\n",
      "GLIP - View: 8 Prompt: 1 inference time per image: 0.5252180999999965\n",
      "GLIP - View: 8 Prompt: 2 inference time per image: 0.5107993000000022\n",
      "GLIP - View: 8 Prompt: 3 inference time per image: 0.4990737999999979\n",
      "GLIP - View: 8 Prompt: 4 inference time per image: 0.5093999999999994\n",
      "GLIP - View: 8 Prompt: 5 inference time per image: 0.5108290000000011\n",
      "GLIP - View: 9 Prompt: 0 inference time per image: 0.5021734999999978\n",
      "GLIP - View: 9 Prompt: 1 inference time per image: 0.5070558000000034\n",
      "GLIP - View: 9 Prompt: 2 inference time per image: 0.5168993999999998\n",
      "GLIP - View: 9 Prompt: 3 inference time per image: 0.5045672000000039\n",
      "GLIP - View: 9 Prompt: 4 inference time per image: 0.5141406000000046\n",
      "GLIP - View: 9 Prompt: 5 inference time per image: 0.5284441999999956\n",
      "GLIP - View: 10 Prompt: 0 inference time per image: 0.5202499000000032\n",
      "GLIP - View: 10 Prompt: 1 inference time per image: 0.5341619000000009\n",
      "GLIP - View: 10 Prompt: 2 inference time per image: 0.5142191999999994\n",
      "GLIP - View: 10 Prompt: 3 inference time per image: 0.5125749000000042\n",
      "GLIP - View: 10 Prompt: 4 inference time per image: 0.5263831999999979\n",
      "GLIP - View: 10 Prompt: 5 inference time per image: 0.5229404999999971\n",
      "GLIP - View: 11 Prompt: 0 inference time per image: 0.5251262999999966\n",
      "GLIP - View: 11 Prompt: 1 inference time per image: 0.5259987999999964\n",
      "GLIP - View: 11 Prompt: 2 inference time per image: 0.5263844999999989\n",
      "GLIP - View: 11 Prompt: 3 inference time per image: 0.5250356000000025\n",
      "GLIP - View: 11 Prompt: 4 inference time per image: 0.5190619999999981\n",
      "GLIP - View: 11 Prompt: 5 inference time per image: 0.5246051000000023\n",
      "Finished GLIP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301d8bae5d4649d598cfe42cfa3d7f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing view: 0, Prompt: 0\n",
      "Processing view: 0, Prompt: 1\n",
      "Processing view: 0, Prompt: 2\n",
      "Processing view: 0, Prompt: 3\n",
      "Processing view: 0, Prompt: 4\n",
      "Processing view: 0, Prompt: 5\n",
      "Processing view: 1, Prompt: 0\n",
      "Processing view: 1, Prompt: 1\n",
      "Processing view: 1, Prompt: 2\n",
      "Processing view: 1, Prompt: 3\n",
      "Processing view: 1, Prompt: 4\n",
      "Processing view: 1, Prompt: 5\n",
      "Processing view: 2, Prompt: 0\n",
      "Processing view: 2, Prompt: 1\n",
      "Processing view: 2, Prompt: 2\n",
      "Processing view: 2, Prompt: 3\n",
      "Processing view: 2, Prompt: 4\n",
      "Processing view: 2, Prompt: 5\n",
      "Processing view: 3, Prompt: 0\n",
      "Processing view: 3, Prompt: 1\n",
      "Processing view: 3, Prompt: 2\n",
      "Processing view: 3, Prompt: 3\n",
      "Processing view: 3, Prompt: 4\n",
      "Processing view: 3, Prompt: 5\n",
      "Processing view: 4, Prompt: 0\n",
      "Processing view: 4, Prompt: 1\n",
      "Processing view: 4, Prompt: 2\n",
      "Processing view: 4, Prompt: 3\n",
      "Processing view: 4, Prompt: 4\n",
      "Processing view: 4, Prompt: 5\n",
      "Processing view: 5, Prompt: 0\n",
      "Processing view: 5, Prompt: 1\n",
      "Processing view: 5, Prompt: 2\n",
      "Processing view: 5, Prompt: 3\n",
      "Processing view: 5, Prompt: 4\n",
      "Processing view: 5, Prompt: 5\n",
      "Processing view: 6, Prompt: 0\n",
      "Processing view: 6, Prompt: 1\n",
      "Processing view: 6, Prompt: 2\n",
      "Processing view: 6, Prompt: 3\n",
      "Processing view: 6, Prompt: 4\n",
      "Processing view: 6, Prompt: 5\n",
      "Processing view: 7, Prompt: 0\n",
      "Processing view: 7, Prompt: 1\n",
      "Processing view: 7, Prompt: 2\n",
      "Processing view: 7, Prompt: 3\n",
      "Processing view: 7, Prompt: 4\n",
      "Processing view: 7, Prompt: 5\n",
      "Processing view: 8, Prompt: 0\n",
      "Processing view: 8, Prompt: 1\n",
      "Processing view: 8, Prompt: 2\n",
      "Processing view: 8, Prompt: 3\n",
      "Processing view: 8, Prompt: 4\n",
      "Processing view: 8, Prompt: 5\n",
      "Processing view: 9, Prompt: 0\n",
      "Processing view: 9, Prompt: 1\n",
      "Processing view: 9, Prompt: 2\n",
      "Processing view: 9, Prompt: 3\n",
      "Processing view: 9, Prompt: 4\n",
      "Processing view: 9, Prompt: 5\n",
      "Processing view: 10, Prompt: 0\n",
      "Processing view: 10, Prompt: 1\n",
      "Processing view: 10, Prompt: 2\n",
      "Processing view: 10, Prompt: 3\n",
      "Processing view: 10, Prompt: 4\n",
      "Processing view: 10, Prompt: 5\n",
      "Processing view: 11, Prompt: 0\n",
      "Processing view: 11, Prompt: 1\n",
      "Processing view: 11, Prompt: 2\n",
      "Processing view: 11, Prompt: 3\n",
      "Processing view: 11, Prompt: 4\n",
      "Processing view: 11, Prompt: 5\n",
      "Predicitons: torch.Size([6804, 6])\n"
     ]
    }
   ],
   "source": [
    "if (object[-1].isdigit()):\n",
    "    config_path = 'configs/demo/' + object[:-2] + '.yaml'\n",
    "else:\n",
    "    config_path = 'configs/demo/' + object + '.yaml'\n",
    "\n",
    "segment(\n",
    "    config_path,\n",
    "    object + '.obj',\n",
    "    'outputs/demo/ABO/' + object + '/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualise Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red :\t back panel\n",
      "Green :\t bottom panel\n",
      "Blue :\t drawers\n",
      "Yellow :\t handles\n",
      "Magenta :\t side panels\n",
      "Cyan :\t top panel\n",
      "Dark Red :\t unknown\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f5dc1570a844fcb73c6176bb418404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(-7.450580â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "colors_dict = {\n",
    "    0: [1, 0, 0],   # Red\n",
    "    1: [0, 1, 0],   # Green\n",
    "    2: [0, 0, 1],   # Blue\n",
    "    3: [1, 1, 0],   # Yellow\n",
    "    4: [1, 0, 1],   # Magenta\n",
    "    5: [0, 1, 1],   # Cyan\n",
    "    6: [0.5, 0, 0], # Dark Red\n",
    "    7: [0, 0.5, 0], # Dark Green\n",
    "    8: [0, 0, 0.5], # Dark Blue\n",
    "    9: [0.5, 0.5, 0.5] # Gray\n",
    "}\n",
    "colors_lst = ['Red', 'Green', 'Blue', 'Yellow', 'Magenta', 'Cyan', 'Dark Red', 'Dark Green', 'Dark Blue', 'Grey']\n",
    "output_file = open('./outputs/demo/ABO/' + object + '/face_preds.json')\n",
    "output = np.array(json.load(output_file))\n",
    "segments = np.unique(output)\n",
    "segments_idx = np.unique(output, return_inverse=True)[1]\n",
    "prompts = segments.tolist()\n",
    "if ('unknown' in prompts):\n",
    "    prompts.remove('unknown')\n",
    "# print('Prompts: ', prompts)\n",
    "colors = np.array([colors_dict[segment_id] for segment_id in segments_idx])\n",
    "for i in range(len(segments)):\n",
    "    print(colors_lst[i],':\\t',segments[i])\n",
    "mp.plot(vertices, faces, colors)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meshseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
