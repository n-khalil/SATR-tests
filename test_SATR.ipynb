{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nkhalil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nkhalil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "c:\\ProgramData\\miniforge3\\envs\\meshseg\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch will run on: cuda:0\n",
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n"
     ]
    }
   ],
   "source": [
    "# Imports and set torch device\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "import torch\n",
    "import sys\n",
    "from scripts.helper_functions import segment\n",
    "import kaolin as kal\n",
    "import matplotlib.pyplot as plt\n",
    "from meshseg.models.GLIP.glip import GLIPModel\n",
    "import igl\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Torch will run on:', device)\n",
    "\n",
    "# obj_path = 'data/demo/penguin.obj'\n",
    "# obj_path = 'data/FAUST/scans/tr_scan_000.obj'\n",
    "# obj_path = 'data/demo/bed.obj'\n",
    "obj_path = 'data/demo/lamp.obj'\n",
    "\n",
    "GM = GLIPModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices:  13704\n",
      "Number of faces:  22736\n"
     ]
    }
   ],
   "source": [
    "# Read mesh\n",
    "mesh = kal.io.obj.import_mesh(\n",
    "    obj_path,\n",
    "    with_normals=True,\n",
    "    with_materials=False,\n",
    ")\n",
    "\n",
    "vertices_tensor = mesh.vertices.to(device)\n",
    "faces_tensor = mesh.faces.to(device)\n",
    "# face_areas = kal.ops.mesh.face_areas(\n",
    "#     vertices_tensor.unsqueeze(0),\n",
    "#     faces_tensor\n",
    "# ).view(len(mesh.faces))\n",
    "\n",
    "vertices = vertices_tensor.detach().cpu().numpy()\n",
    "faces = faces_tensor.detach().cpu().numpy()\n",
    "\n",
    "print('Number of vertices: ', vertices.shape[0])\n",
    "print('Number of faces: ', faces.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b777a8b29f4c47a0947cc870edffb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(-9.685754â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize mesh\n",
    "mp.plot(vertices, faces)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample of rendered images\n",
    "# gen = torch.Generator()\n",
    "# gen.seed()\n",
    "# std = 4\n",
    "# center_elev = 0\n",
    "# center_azim = 3.14\n",
    "# elev = torch.randn(1, generator=gen) * np.pi / std + center_elev\n",
    "# azim = torch.randn(1, generator=gen) * 2 * np.pi / std + center_azim\n",
    "# r = 2\n",
    "# x = r * torch.cos(elev) * torch.cos(azim)\n",
    "# y = r * torch.sin(elev)\n",
    "# z = r * torch.cos(elev) * torch.sin(azim)\n",
    "# pos = torch.tensor([x, y, z]).unsqueeze(0).to(device)\n",
    "# center = vertices_tensor.mean(dim = 0).to(device)\n",
    "# look_at = center-pos\n",
    "# # look_at = -pos\n",
    "# direction = torch.tensor([0.0, 1.0, 0.0]).unsqueeze(0).to(device)\n",
    "# camera_transform = kal.render.camera.generate_transformation_matrix(pos, look_at, direction).to(device)\n",
    "# lights=torch.tensor([1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "# lights = lights.unsqueeze(0).to(device)\n",
    "# background = torch.tensor([0.0, 0.0, 0.0]).to(device)\n",
    "\n",
    "# (\n",
    "#     face_vertices_camera,\n",
    "#     face_vertices_image,\n",
    "#     face_normals,\n",
    "# ) = kal.render.mesh.prepare_vertices(\n",
    "#     mesh.vertices.to(device),\n",
    "#     mesh.faces.to(device),\n",
    "#     kal.render.camera.generate_perspective_projection(np.pi / 3).to(device),\n",
    "#     camera_transform=camera_transform,\n",
    "# )\n",
    "\n",
    "# face_attributes = kal.ops.mesh.index_vertices_by_faces(\n",
    "#             torch.ones(1, len(mesh.vertices), 3).to(device)\n",
    "#             * torch.tensor([0.5, 0.5, 0.5]).unsqueeze(0).unsqueeze(0).to(device),\n",
    "#             faces_tensor,\n",
    "#         ).to(device)\n",
    "# face_attributes = [\n",
    "#                 face_attributes,  # Colors\n",
    "#                 torch.ones((1, faces.shape[0], 3, 1), device=device),  # hard seg. mask\n",
    "#             ]\n",
    "# image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n",
    "#     1024,\n",
    "#     1024,\n",
    "#     face_vertices_camera[:, :, :, -1],\n",
    "#     face_vertices_image,\n",
    "#     face_attributes,\n",
    "#     face_normals[:, :, -1],\n",
    "# )\n",
    "# image_features, mask = image_features\n",
    "# image_normals = face_normals[:, face_idx].squeeze(0)\n",
    "\n",
    "# image = torch.clamp(image_features, 0.0, 1.0)\n",
    "\n",
    "# image_lighting = kal.render.mesh.spherical_harmonic_lighting(\n",
    "#     image_normals, lights\n",
    "# ).unsqueeze(0)\n",
    "\n",
    "# image = image * image_lighting.repeat(1, 3, 1, 1).permute(\n",
    "#     0, 2, 3, 1\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "# background_mask = torch.zeros(image.shape).to(device)\n",
    "# mask = mask.squeeze(-1)\n",
    "# background_idx = torch.where(mask == 0)\n",
    "# assert torch.all(\n",
    "#     image[background_idx] == torch.zeros(3).to(device)\n",
    "# )  # Remove it may be taking a lot of time\n",
    "# background_mask[\n",
    "#     background_idx\n",
    "# ] = background  # .repeat(background_idx[0].shape)\n",
    "# image = torch.clamp(image + background_mask, 0.0, 1.0).squeeze()\n",
    "\n",
    "# plt.imshow(image.cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "# image = (image * 255).cpu().numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmPrediction = GM.predict(image, 'body, lamp')\n",
    "# print(gmPrediction[1])\n",
    "# plt.figure(figsize=[8, 6])\n",
    "# plt.imshow(gmPrediction[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/demo\\lamp.obj\n",
      "['the base of a lamp.', 'the body of a lamp.', 'the lampshade of a lamp.', 'the neck of a lamp.']\n",
      "{'base': 0, 'body': 1, 'lampshade': 2, 'neck': 3, 'unknown': 4}\n",
      "Reading the mesh...\n",
      "Reading the mesh with path: data/demo\\lamp.obj\n",
      "\thaving 22736 faces and 13704 vertices\n",
      "Sampling the mesh...\n",
      "Creating the renderer...\n",
      "Random rendering: False\n",
      "Rendering the views...\n",
      "Rendering the views...done\n",
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n"
     ]
    }
   ],
   "source": [
    "segment(\n",
    "    'configs/demo/lamp.yaml',\n",
    "    'lamp.obj',\n",
    "    'outputs/demo/ABO/lamp/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = open('./outputs/demo/ABO/lamp/face_preds.json')\n",
    "output = np.array(json.load(output_file))\n",
    "colors = np.unique(output, return_inverse=True)[1]\n",
    "print('Prompts: ', np.unique(output))\n",
    "mp.plot(vertices, faces, colors)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meshseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
