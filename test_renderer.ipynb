{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and set torch device\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import kaolin as kal\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Torch will run on:', device)\n",
    "\n",
    "object = 'bed' \n",
    "obj_path = 'data/demo/' + object + '.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read mesh\n",
    "mesh = kal.io.obj.import_mesh(\n",
    "    obj_path,\n",
    "    with_normals=True,\n",
    "    with_materials=False,\n",
    ")\n",
    "\n",
    "vertices_tensor = mesh.vertices.to(device)\n",
    "faces_tensor = mesh.faces.to(device)\n",
    "\n",
    "vertices = vertices_tensor.detach().cpu().numpy()\n",
    "faces = faces_tensor.detach().cpu().numpy()\n",
    "color =  mesh.vertex_normals.cpu().numpy()\n",
    "\n",
    "print('Number of vertices: ', vertices.shape[0])\n",
    "print('Number of faces: ', faces.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mesh\n",
    "mp.plot(vertices, faces, color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshseg.renderer.renderer import Renderer\n",
    "from meshseg.mesh import Mesh\n",
    "\n",
    "myMesh = Mesh('data/demo/' + object + '.obj')\n",
    "\n",
    "render = Renderer(dim=(1024, 1024))\n",
    "rendered_images, elev, azim, _, faces_idx = render.render_views(\n",
    "            None,\n",
    "            None,\n",
    "            2,\n",
    "            myMesh,\n",
    "            num_views=1,\n",
    "            show=False,\n",
    "            center_azim=3.14,\n",
    "            center_elev=0,\n",
    "            std=4,\n",
    "            return_views=True,\n",
    "            return_mask=True,\n",
    "            return_face_idx=True,\n",
    "            lighting=True,\n",
    "            background=torch.tensor([255.,255.,255.]).to(device),\n",
    "            seed=2023,\n",
    "            random_rendering=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_idx_np = faces_idx[0].flatten().cpu().int().numpy()\n",
    "faces_idx_np = faces_idx_np[faces_idx_np != -1]\n",
    "print(faces_idx_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_from_view2(elev, azim, r=3.0):\n",
    "    x = r * torch.cos(elev) * torch.cos(azim)\n",
    "    y = r * torch.sin(elev)\n",
    "    z = r * torch.cos(elev) * torch.sin(azim)\n",
    "    pos = torch.tensor([x, y, z]).unsqueeze(0)\n",
    "    look_at = -pos\n",
    "    direction = torch.tensor([0.0, 1.0, 0.0]).unsqueeze(0)\n",
    "\n",
    "    camera_proj = kal.render.camera.generate_transformation_matrix(pos, look_at, direction)\n",
    "    return camera_proj\n",
    "\n",
    "\n",
    "def render_view(elev, azim, r):\n",
    "    background = torch.tensor([255.0, 255.0, 255.0]).to(device)\n",
    "    # face_attributes = kal.ops.mesh.index_vertices_by_faces(\n",
    "    #     torch.ones(1, len(mesh.vertices), 3).to(device)\n",
    "    #     * torch.tensor([0.5, 0.5, 0.5]).unsqueeze(0).unsqueeze(0).to(device),\n",
    "    #     faces_tensor,\n",
    "    # )\n",
    "    face_attributes = kal.ops.mesh.index_vertices_by_faces(\n",
    "            mesh.vertex_normals.unsqueeze(0).to(device),\n",
    "            faces_tensor\n",
    "    ) \n",
    "    face_attributes = [\n",
    "        face_attributes,  # Colors\n",
    "        torch.ones((1, faces.shape[0], 3, 1), device=device),  # hard seg. mask\n",
    "    ]\n",
    "    \n",
    "    camera_projection = kal.render.camera.generate_perspective_projection(np.pi / 3).to(\n",
    "                    device\n",
    "                )\n",
    "    camera_transform = get_camera_from_view2(elev, azim, r=r).to(device)\n",
    "    (\n",
    "        face_vertices_camera,\n",
    "        face_vertices_image,\n",
    "        face_normals,\n",
    "    ) = kal.render.mesh.prepare_vertices(\n",
    "        mesh.vertices.to(device),\n",
    "        mesh.faces.to(device),\n",
    "        camera_projection,\n",
    "        camera_transform=camera_transform,\n",
    "    )\n",
    "\n",
    "    image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n",
    "        1024,\n",
    "        1024,\n",
    "        face_vertices_camera[:, :, :, -1],\n",
    "        face_vertices_image,\n",
    "        face_attributes,\n",
    "        face_normals[:, :, -1],\n",
    "    )\n",
    "    image_features, mask = image_features\n",
    "    image = torch.clamp(image_features, 0.0, 1.0)\n",
    "    lights=torch.tensor([1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]).unsqueeze(0).to(device)\n",
    "    image_normals = face_normals[:, face_idx].squeeze(0)\n",
    "    image_lighting = kal.render.mesh.spherical_harmonic_lighting(\n",
    "        image_normals, lights\n",
    "        ).unsqueeze(0)\n",
    "    image = image * image_lighting.repeat(1, 3, 1, 1).permute(\n",
    "        0, 2, 3, 1\n",
    "        ).to(device)\n",
    "    image = torch.clamp(image, 0.0, 1.0)\n",
    "    background_mask = torch.zeros(image.shape).to(device)\n",
    "    mask = mask.squeeze(-1)\n",
    "    background_idx = torch.where(mask == 0)\n",
    "    assert torch.all(\n",
    "        image[background_idx] == torch.zeros(3).to(device)\n",
    "    )  # Remvoe it may be taking a lot of time\n",
    "    background_mask[\n",
    "        background_idx\n",
    "    ] = background  # .repeat(background_idx[0].shape)\n",
    "    image = torch.clamp(image + background_mask, 0.0, 1.0)\n",
    "    image = image.squeeze().cpu().numpy()\n",
    "    image *= 255.0\n",
    "    image = image.astype(np.uint8)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAM import SamPredictor, sam_model_registry\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"./SAM/MODEL/sam_vit_h_4b8939.pth\")\n",
    "predictor = SamPredictor(sam)\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = torch.deg2rad(torch.tensor(45)).to(device)\n",
    "azim = torch.deg2rad(torch.tensor(45)).to(device)\n",
    "image = render_view(elev, azim, 4)\n",
    "predictor.set_image(image)\n",
    "input_box = np.array([190, 250, 800, 740])\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=True,\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "show_box(input_box, plt.gca())\n",
    "show_mask(masks[0], plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = torch.deg2rad(torch.tensor(10)).to(device)\n",
    "azim = torch.deg2rad(torch.tensor(90)).to(device)\n",
    "image = render_view(elev, azim, 3)\n",
    "predictor.set_image(image)\n",
    "input_box = np.array([170, 320, 860, 580])\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=True,\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "show_box(input_box, plt.gca())\n",
    "show_mask(masks[0], plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = torch.deg2rad(torch.tensor(10)).to(device)\n",
    "azim = torch.deg2rad(torch.tensor(90)).to(device)\n",
    "image = render_view(elev, azim, 3)\n",
    "predictor.set_image(image)\n",
    "input_box = np.array([130, 540, 170, 620])\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=True,\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "show_box(input_box, plt.gca())\n",
    "show_mask(masks[0], plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = torch.deg2rad(torch.tensor(20)).to(device)\n",
    "azim = torch.deg2rad(torch.tensor(150)).to(device)\n",
    "image = render_view(elev, azim, 3)\n",
    "predictor.set_image(image)\n",
    "input_box = np.array([10, 150, 420, 570])\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=True,\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "show_box(input_box, plt.gca())\n",
    "show_mask(masks[0], plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = 'bed' \n",
    "obj_path = 'data/demo/' + object + '.obj'\n",
    "# Read mesh\n",
    "mesh = kal.io.obj.import_mesh(\n",
    "    obj_path,\n",
    "    with_normals=True,\n",
    "    with_materials=False,\n",
    ")\n",
    "\n",
    "vertices = mesh.vertices.cpu().numpy()\n",
    "faces = mesh.faces.cpu().numpy()\n",
    "\n",
    "elev = torch.deg2rad(torch.tensor(40)).to(device)\n",
    "azim = torch.deg2rad(torch.tensor(120)).to(device)\n",
    "r = 2\n",
    "x = r * torch.cos(elev) * torch.cos(azim)\n",
    "y = r * torch.sin(elev)\n",
    "z = r * torch.cos(elev) * torch.sin(azim)\n",
    "\n",
    "pos = torch.tensor([x, y, z]).unsqueeze(0)\n",
    "look_at = torch.mean(mesh.vertices, axis=0) - pos\n",
    "direction = torch.tensor([0.0, 1.0, 0.0]).unsqueeze(0)\n",
    "\n",
    "camera_transform = kal.render.camera.generate_transformation_matrix(pos, look_at, direction)\n",
    "camera_projection = kal.render.camera.generate_perspective_projection(np.pi / 3).to(device)\n",
    "(\n",
    "    face_vertices_camera,\n",
    "    face_vertices_image,\n",
    "    face_normals,\n",
    ") = kal.render.mesh.prepare_vertices(\n",
    "    mesh.vertices.to(device),\n",
    "    mesh.faces.to(device),\n",
    "    camera_projection,\n",
    "    camera_transform=camera_transform,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meshseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
